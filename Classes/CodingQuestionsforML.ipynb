{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPZoJP+UsNwvtVsJJlia5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushhaabhhh/ML-learning/blob/main/CodingQuestionsforML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xp7Uou46-G3b"
      },
      "outputs": [],
      "source": [
        "# 1. Write a Python function to calculate accuracy using true and predicted labels.\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate accuracy based on true and predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (list or numpy array): True labels.\n",
        "        y_pred (list or numpy array): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy score (in percentage).\n",
        "    \"\"\"\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length of true labels and predicted labels must be the same.\")\n",
        "\n",
        "    correct_predictions = sum([\n",
        "        1 for true,\n",
        "        pred in zip(y_true, y_pred)  # Combines the true and predicted labels into pairs\n",
        "        if true == pred              # Compares each pair of true and predicted labels. If they match, it counts as a correct prediction\n",
        "    ])\n",
        "    accuracy = (correct_predictions / len(y_true)) * 100  # Percentage format\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Sample true and predicted labels\n",
        "y_true = [0, 1, 1, 0, 1, 1, 0]\n",
        "y_pred = [0, 1, 0, 0, 1, 1, 1]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = calculate_accuracy(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJNf5x49-VHH",
        "outputId": "b26a494a-38e0-432f-f19e-701725ab7683"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.43 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Implement a function that computes the sigmoid activation.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid activation for a given input.\n",
        "\n",
        "    Parameters:\n",
        "        x (float, int, or numpy array): Input value(s) to compute the sigmoid.\n",
        "\n",
        "    Returns:\n",
        "        numpy array or float: Sigmoid activation value(s).\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x)) # Formula based answer"
      ],
      "metadata": {
        "id": "TcvILF6J_Q7Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Single value\n",
        "x = 0.5\n",
        "print(f\"Sigmoid({x}) = {sigmoid(x)}\")\n",
        "\n",
        "# Array of values\n",
        "x_array = np.array([-1, 0, 1, 2])\n",
        "print(f\"Sigmoid({x_array}) = {sigmoid(x_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLyUTPA8_VCX",
        "outputId": "179ce589-f677-49c5-a419-2308687a969a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid(0.5) = 0.6224593312018546\n",
            "Sigmoid([-1  0  1  2]) = [0.26894142 0.5        0.73105858 0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a function to calculate TP, FP, TN, and FN from true and predicted labels.\n",
        "\n",
        "def calculate_confusion_matrix_elements(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate True Positives (TP), False Positives (FP),\n",
        "    True Negatives (TN), and False Negatives (FN).\n",
        "\n",
        "    Parameters:\n",
        "        y_true (list or numpy array): True labels (binary: 0 or 1).\n",
        "        y_pred (list or numpy array): Predicted labels (binary: 0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing TP, FP, TN, and FN.\n",
        "    \"\"\"\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length of true labels and predicted labels must be the same.\")\n",
        "\n",
        "    # Initialize counts\n",
        "    TP = FP = TN = FN = 0\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        if true == 1 and pred == 1:\n",
        "            TP += 1                             # True Positive\n",
        "        elif true == 0 and pred == 1:\n",
        "            FP += 1                             # False Positive\n",
        "        elif true == 0 and pred == 0:\n",
        "            TN += 1                             # True Negative\n",
        "        elif true == 1 and pred == 0:\n",
        "            FN += 1                             # False Negative\n",
        "\n",
        "    return {\"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN}"
      ],
      "metadata": {
        "id": "RDs-omv5_fyy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1]\n",
        "\n",
        "confusion_matrix = calculate_confusion_matrix_elements(y_true, y_pred)\n",
        "print(\"Confusion Matrix Elements : \")\n",
        "print(f\"TP : {confusion_matrix['TP']}\")\n",
        "print(f\"FP : {confusion_matrix['FP']}\")\n",
        "print(f\"TN : {confusion_matrix['TN']}\")\n",
        "print(f\"FN : {confusion_matrix['FN']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikQmEHrj_gUQ",
        "outputId": "93d96b1f-9274-4379-a55a-3228c08f24ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix Elements:\n",
            "TP: 4\n",
            "FP: 1\n",
            "TN: 3\n",
            "FN: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Implement a Python function to calculate the Precision, Recall, and F1-Score from TP, FP, TN, and FN.\n",
        "\n",
        "def precision_recall_f1(tp, fp, fn):\n",
        "    \"\"\"\n",
        "    Calculate Precision, Recall, and F1-Score.\n",
        "\n",
        "    Parameters:\n",
        "        tp (int): True Positives.\n",
        "        fp (int): False Positives.\n",
        "        fn (int): False Negatives.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing Precision, Recall, and F1-Score.\n",
        "    \"\"\"\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1_score\n",
        "    }"
      ],
      "metadata": {
        "id": "z3H7WneT_oeZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "confusion_matrix = {\"TP\": 50, \"FP\": 10, \"TN\": 30, \"FN\": 5}\n",
        "metrics = precision_recall_f1(confusion_matrix[\"TP\"], confusion_matrix[\"FP\"], confusion_matrix[\"FN\"])\n",
        "\n",
        "print(f\"Precision: {metrics['Precision']:.2f}\")\n",
        "print(f\"Recall: {metrics['Recall']:.2f}\")\n",
        "print(f\"F1-Score: {metrics['F1-Score']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGPmcTK-AMjc",
        "outputId": "1244a187-9bdb-4d69-b712-4d4af7ca2299"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.83\n",
            "Recall: 0.91\n",
            "F1-Score: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Implement Linear Regression from scratch in Python using NumPy.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute the predictions\n",
        "def predict(X, m, b):\n",
        "    return m * X + b\n",
        "\n",
        "# Function to compute the slope (m) and intercept (b)\n",
        "def linear_regression(X, y):\n",
        "    # Number of data points\n",
        "    n = len(X)\n",
        "\n",
        "    # Calculate the slope (m) using the normal equation\n",
        "    m = (n * np.sum(X * y) - np.sum(X) * np.sum(y)) / (n * np.sum(X**2) - np.sum(X)**2)\n",
        "\n",
        "    # Calculate the intercept (b)\n",
        "    b = (np.sum(y) - m * np.sum(X)) / n\n",
        "\n",
        "    return m, b"
      ],
      "metadata": {
        "id": "QSlXoDmxCW5R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "X = np.array([1, 2, 3, 4, 5])  # Feature (input) values\n",
        "y = np.array([1.5, 3.1, 4.5, 6.0, 7.8])  # Target (output) values\n",
        "\n",
        "# Step 1: Calculate the coefficients (m and b)\n",
        "m, b = linear_regression(X, y)\n",
        "\n",
        "# Step 2: Predict the output using the learned model\n",
        "y_pred = predict(X, m, b)\n",
        "\n",
        "# Output the results\n",
        "print(\"Slope (m):\", m)\n",
        "print(\"Intercept (b):\", b)\n",
        "print(\"Predicted Values:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGWLy6gDwTp",
        "outputId": "0917c2cf-8110-435a-907f-646ed646e846"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): 1.55\n",
            "Intercept (b): -0.07000000000000028\n",
            "Predicted Values: [1.48 3.03 4.58 6.13 7.68]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Implement Logistic Regression from scratch using gradient descent in Python.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights with zeros\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "\n",
        "        # Gradient descent loop\n",
        "        for _ in range(self.epochs):\n",
        "            model = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "            dw = (1 / len(X)) * np.dot(X.T, (model - y))  # Gradient for weights\n",
        "            db = (1 / len(X)) * np.sum(model - y)         # Gradient for bias\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict binary labels\n",
        "        return (self.sigmoid(np.dot(X, self.weights) + self.bias) > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "gZzxFuGuAjRJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "model = LogisticRegression(lr=0.1, epochs=1000)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "print(\"Weights :\", model.weights)\n",
        "print(\"Bias :\", model.bias)\n",
        "print(\"Predictions :\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Eg_DY9BTd0",
        "outputId": "1dbb11b3-b7db-4589-ef44-cd40e30178ad"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights :  [ 3.67640108 -1.17371262]\n",
            "Bias :  -4.850113702579488\n",
            "Predictions : [0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Implement all the evaluations metrices for both classification and regression.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(y_true, y_pred, task='classification'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using multiple metrics for classification or regression tasks.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (numpy array): True target values.\n",
        "        y_pred (numpy array): Predicted values by the model.\n",
        "        task (str): Either 'classification' or 'regression'. Default is 'classification'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the calculated metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty dictionary to store evaluation results\n",
        "    metrics = {}\n",
        "\n",
        "    if task == 'regression':\n",
        "        # Regression metrics\n",
        "        # Mean Squared Error (MSE)\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        metrics['MSE'] = mse\n",
        "\n",
        "        # Mean Absolute Error (MAE)\n",
        "        mae = np.mean(np.abs(y_true - y_pred))\n",
        "        metrics['MAE'] = mae\n",
        "\n",
        "        # Root Mean Squared Error (RMSE)\n",
        "        rmse = np.sqrt(mse)\n",
        "        metrics['RMSE'] = rmse\n",
        "\n",
        "        # R-squared (R²)\n",
        "        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        ss_residual = np.sum((y_true - y_pred) ** 2)\n",
        "        r2 = 1 - (ss_residual / ss_total)\n",
        "        metrics['R²'] = r2\n",
        "\n",
        "        # Adjusted R-squared (adjusted R²)\n",
        "        n = len(y_true)  # Number of samples\n",
        "        p = len(y_pred[0]) if len(y_pred.shape) > 1 else 1  # Number of features\n",
        "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "        metrics['Adjusted R²'] = adj_r2\n",
        "\n",
        "    elif task == 'classification':\n",
        "        # Classification metrics\n",
        "        # Confusion Matrix\n",
        "        tp = np.sum((y_true == 1) & (y_pred == 1))  # True Positive\n",
        "        tn = np.sum((y_true == 0) & (y_pred == 0))  # True Negative\n",
        "        fp = np.sum((y_true == 0) & (y_pred == 1))  # False Positive\n",
        "        fn = np.sum((y_true == 1) & (y_pred == 0))  # False Negative\n",
        "\n",
        "        confusion = {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "        metrics['Confusion Matrix'] = confusion\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        metrics['Accuracy'] = accuracy\n",
        "\n",
        "        # Precision\n",
        "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "        metrics['Precision'] = precision\n",
        "\n",
        "        # Recall\n",
        "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        metrics['Recall'] = recall\n",
        "\n",
        "        # F1 Score\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "        metrics['F1 Score'] = f1\n",
        "\n",
        "        # F-beta Score (choose beta = 1 for F1, or adjust accordingly)\n",
        "        beta = 1  # You can change this value\n",
        "        f_beta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) != 0 else 0\n",
        "        metrics['F-Beta Score'] = f_beta\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "hjxAlDkXFmKX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "\n",
        "# For regression:\n",
        "y_true_reg = np.array([3.0, -0.5, 2.0, 7.0])\n",
        "y_pred_reg = np.array([2.5, 0.0, 2.1, 7.8])\n",
        "\n",
        "regression_metrics = evaluate_model(y_true_reg, y_pred_reg, task='regression')\n",
        "print(\"Regression Metrics:\", regression_metrics)\n",
        "\n",
        "# For classification:\n",
        "y_true_class = np.array([1, 0, 1, 1, 0])\n",
        "y_pred_class = np.array([1, 0, 1, 0, 0])\n",
        "\n",
        "classification_metrics = evaluate_model(y_true_class, y_pred_class, task='classification')\n",
        "print(\"Classification Metrics:\", classification_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r5jXEAPFsXM",
        "outputId": "316bc387-66e6-4c5e-98b9-7ed6c38e4ae5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Metrics: {'MSE': 0.2874999999999999, 'MAE': 0.475, 'RMSE': 0.5361902647381803, 'R²': 0.9605995717344754, 'Adjusted R²': 0.9408993576017131}\n",
            "Classification Metrics: {'Confusion Matrix': {'TP': 2, 'TN': 2, 'FP': 0, 'FN': 1}, 'Accuracy': 0.8, 'Precision': 1.0, 'Recall': 0.6666666666666666, 'F1 Score': 0.8, 'F-Beta Score': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Implement Gradient Descent for Linear Regression from Scratch, the goal it so minimise the Mean Squared Error (MSE) loss function.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Calculate the model's predictions\n",
        "            predictions = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Compute the loss (Mean Squared Error)\n",
        "            loss = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2)\n",
        "\n",
        "            # Calculate gradients (derivatives of loss w.r.t weights and bias)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update the weights and bias using gradient descent\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
        "                print(f\"Epoch {epoch}/{self.epochs} - Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Example Usage\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.5, 6.0, 7.8])\n",
        "\n",
        "model = LinearRegressionGD(lr=0.01, epochs=1000)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewhfLXsbFtG7",
        "outputId": "6099da1b-4419-402f-9824-527e6b8ba2cd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 - Loss: 12.895\n",
            "Epoch 100/1000 - Loss: 0.018253616915606008\n",
            "Epoch 200/1000 - Loss: 0.014247849883377524\n",
            "Epoch 300/1000 - Loss: 0.011392047815031099\n",
            "Epoch 400/1000 - Loss: 0.00935608174634119\n",
            "Epoch 500/1000 - Loss: 0.007904595357000166\n",
            "Epoch 600/1000 - Loss: 0.006869797788002472\n",
            "Epoch 700/1000 - Loss: 0.006132067130197486\n",
            "Epoch 800/1000 - Loss: 0.005606122211335127\n",
            "Epoch 900/1000 - Loss: 0.005231164149404919\n",
            "Predictions: [1.54175053 3.0680942  4.59443787 6.12078154 7.64712521]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Logistic Regression Cost Function and Gradient Calculation.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegressionGD:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def cost_function(self, y_true, y_pred):\n",
        "        # Cross-entropy loss\n",
        "        m = len(y_true)\n",
        "        cost = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return cost\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Linear model and apply sigmoid\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(linear_model)\n",
        "\n",
        "            # Compute the cost\n",
        "            cost = self.cost_function(y, y_pred)\n",
        "\n",
        "            # Calculate gradients (derivatives of the cost w.r.t weights and bias)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update the weights and bias using gradient descent\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            if epoch % 100 == 0:  # Print cost every 100 epochs\n",
        "                print(f\"Epoch {epoch}/{self.epochs} - Cost: {cost}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict binary labels (0 or 1)\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return np.where(self.sigmoid(linear_model) > 0.5, 1, 0)\n",
        "\n",
        "# Example Usage\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "model = LogisticRegressionGD(lr=0.1, epochs=1000)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy4HBpSsGeT7",
        "outputId": "2a91dbdc-7938-477f-a8b9-fccbea77d7ad"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 - Cost: 0.6931471805599454\n",
            "Epoch 100/1000 - Cost: 0.3847939539214676\n",
            "Epoch 200/1000 - Cost: 0.2994013400600758\n",
            "Epoch 300/1000 - Cost: 0.24908227946519595\n",
            "Epoch 400/1000 - Cost: 0.2160872290725683\n",
            "Epoch 500/1000 - Cost: 0.19264855679411716\n",
            "Epoch 600/1000 - Cost: 0.1749954433707084\n",
            "Epoch 700/1000 - Cost: 0.16111024844568422\n",
            "Epoch 800/1000 - Cost: 0.1498230045636801\n",
            "Epoch 900/1000 - Cost: 0.14040974044624052\n",
            "Predictions: [0 0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Polynomial Regression (Using Gradient Descent).\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class PolynomialRegressionGD:\n",
        "    def __init__(self, degree=2, lr=0.01, epochs=1000):\n",
        "        self.degree = degree\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def polynomial_features(self, X):\n",
        "        # Generate polynomial features for X (e.g., X^1, X^2, ..., X^degree)\n",
        "        poly_X = np.hstack([X ** i for i in range(1, self.degree + 1)])\n",
        "        return poly_X\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.polynomial_features(X)\n",
        "        n_samples, n_features = X_poly.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Compute predictions\n",
        "            predictions = np.dot(X_poly, self.weights) + self.bias\n",
        "\n",
        "            # Compute the loss (Mean Squared Error)\n",
        "            loss = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2)\n",
        "\n",
        "            # Calculate gradients (derivatives of loss w.r.t weights and bias)\n",
        "            dw = (1 / n_samples) * np.dot(X_poly.T, (predictions - y))\n",
        "            db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights and bias using gradient descent\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
        "                print(f\"Epoch {epoch}/{self.epochs} - Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.polynomial_features(X)\n",
        "        return np.dot(X_poly, self.weights) + self.bias\n",
        "\n",
        "# Example Usage\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.5, 3.1, 4.5, 6.0, 7.8])\n",
        "\n",
        "model = PolynomialRegressionGD(degree=2, lr=0.01, epochs=1000)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pUx27m5Gl6z",
        "outputId": "31c92d1c-2470-46b5-cc0b-b73aea7b5c5a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 - Loss: 12.895\n",
            "Epoch 100/1000 - Loss: 6426556.05898684\n",
            "Epoch 200/1000 - Loss: 3365718235529.9434\n",
            "Epoch 300/1000 - Loss: 1.762695183401857e+18\n",
            "Epoch 400/1000 - Loss: 9.231593651507521e+23\n",
            "Epoch 500/1000 - Loss: 4.834773598353066e+29\n",
            "Epoch 600/1000 - Loss: 2.5320693944879988e+35\n",
            "Epoch 700/1000 - Loss: 1.3260963079402075e+41\n",
            "Epoch 800/1000 - Loss: 6.945036426571692e+46\n",
            "Epoch 900/1000 - Loss: 3.637257013506682e+52\n",
            "Predictions: [-1.70017739e+28 -5.96638465e+28 -1.28736423e+29 -2.24219504e+29\n",
            " -3.46113088e+29]\n"
          ]
        }
      ]
    }
  ]
}